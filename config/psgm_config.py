# config/psgm_config.py

import torch

# --- General Configuration ---
# Device to run the models on. Automatically detects CUDA if available.
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
# Random seed for reproducibility. (Note: PSGM code does not explicitly use random.seed for this)
SEED = 42 

# --- Path Configurations ---
# Path to the combined peptide-HLA raw data.
TRAIN_DATA_PATH = "data/raw/hebing.CSV"
# Path to the HLA pseudo-sequence database.
HLA_DB_PATH = "data/raw/伪序列数据.CSV"

# Paths to save processed data (generated by train.py).
PROCESSED_TRAIN_DATA_PATH = "data/processed/train.csv"
PROCESSED_VAL_DATA_PATH = "data/processed/val.csv"
PROCESSED_TEST_DATA_PATH = "data/processed/test.csv"

# Path to save the trained HLAGenerator model checkpoint.
MODEL_SAVE_PATH = "models/psgm_checkpoints/hla_generator.pth"
# Path to save the generation results (generated pseudo-sequences and matches).
RESULTS_SAVE_PATH = "results/generation_results_GAN.csv" # Adjusted to a more logical 'results' folder

# --- Data Parameters ---
# Maximum length for peptide sequences.
PEPTIDE_MAX_LEN = 14
# Fixed length for HLA pseudo-sequences.
HLA_PSEUDO_SEQUENCE_LEN = 34

# --- Model Parameters (HLAGenerator) ---
# ESM-2 model path. (Note: Your PSGM code uses `esm.pretrained.esm2_t30_150M_UR50D()` directly,
# but it's good practice to have a local path config if you ever download it.)
# LOCAL_ESM_MODEL_PATH = "models/esm2_t30_150M_UR50D" # Uncomment if you want to use local ESM model
ESM2_DIM = 640 # Embedding dimension of ESM-2 model (t30_150M_UR50D)
EMBED_DIM = 256 # Embedding dimension for Generator's internal layers
GENERATOR_DEC_LAYERS = 6 # Number of decoder layers in Generator
GENERATOR_NHEAD = 8 # Number of attention heads in Generator's Transformer

# --- Model Parameters (Discriminator) ---
DISCRIMINATOR_EMBED_DIM = 256 # Embedding dimension for Discriminator
DISCRIMINATOR_NUM_LAYERS = 3 # Number of encoder layers in Discriminator
DISCRIMINATOR_NHEAD = 8 # Number of attention heads in Discriminator's Transformer

# --- Training Parameters ---
# Batch size for training.
BATCH_SIZE = 128
# Number of training epochs.
NUM_EPOCHS = 10
# Learning rate for both Generator and Discriminator optimizers.
LEARNING_RATE = 1e-7
# Weight for the adversarial loss component in Generator's total loss.
LAMBDA_ADV = 0.3
# Patience for early stopping during training.
PATIENCE = 3
# Minimum change in validation loss to be considered an improvement for early stopping.
MIN_DELTA = 0.001
# Batch size for validation loader.
VAL_BATCH_SIZE_MULTIPLIER = 2 # e.g., BATCH_SIZE * 2

# --- Generation Parameters ---
# Top-p sampling probability threshold for sequence generation.
GENERATION_TOP_P = 0.9
# Temperature for sequence generation.
GENERATION_TEMPERATURE = 1.0
# Number of nearest neighbors to query in HLAMapper.
NN_NUM_NEIGHBORS = 50
# Number of top matches to return from HLAMapper query.
TOP_MATCHES_TO_RETURN = 50

# --- Vocab Parameters ---
PAD_TOKEN = '[PAD]'
MASK_TOKEN = '[MASK]'
AMINO_ACIDS = "ACDEFGHIKLMNPQRSTVWY"
