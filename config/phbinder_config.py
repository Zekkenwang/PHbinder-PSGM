import torch

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
SEED = 42
LOCAL_ESM_MODEL_PATH = "models/esm2_t30_150M_UR50D"
SAVE_PATH_LORA_WEIGHTS = "models/phbinder_pretrain"
SAVE_PATH_MAIN_MODEL_CHECKPOINTS = "models/phbinder"
TRAIN_DATA_PATH = "data/raw/HLA_I_epitope_train_shuffle.csv"
VALIDATION_DATA_PATH = "data/raw/HLA_I_epitope_validation.csv"
TEST_DATA_PATH = "data/raw/HLA_I_epitope_test.csv"
EXTERNAL_DATA_PATH = "data/external/phbinder/external_data.CSV"
PREDICTION_OUTPUT_PATH = "data/external/phbinder/output.CSV"
EPITOPE_MAX_LEN = 16
LORA_TARGET_MODULES = ['query', 'key', 'value', 'out_proj']
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.1
LORA_TASK_TYPE = "FEATURE_EXTRACTION"
LORA_LEARNING_RATE = 1e-5
LORA_NUM_EPOCHS = 10
LORA_PATIENCE = 5
LORA_BATCH_SIZE = 32
TRANSFORMER_N_LAYERS = 6
TRANSFORMER_N_HEAD = 16
TRANSFORMER_D_MODEL = 640
TRANSFORMER_D_FF = 64
CNN_NUM_CHANNEL = 256
CNN_REGION_EMBEDDING_SIZE = 3
CNN_KERNEL_SIZE = 3
CNN_PADDING_SIZE = 1
CNN_STRIDE = 1
CNN_POOLING_SIZE = 2
TRANSFORMER_DROPOUT = 0.2
FC_TASK_HIDDEN_SIZE_1 = TRANSFORMER_D_MODEL // 4 
FC_TASK_HIDDEN_SIZE_2 = 64
FC_TASK_DROPOUT = 0.3
NUM_CLASSES = 2
MAIN_MODEL_LEARNING_RATE = 5e-6
MAIN_MODEL_WEIGHT_DECAY = 0.0025
MAIN_MODEL_NUM_EPOCHS = 1000
MAIN_MODEL_PATIENCE = 5
MAIN_MODEL_BATCH_SIZE = 64
LOSS_OFFSET = 0.04
