# scripts/train_psgm.py

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import random

# Import configurations
from config.psgm_config import Config

# Import modules from psgm/
from src.psgm.vocab import vocab # Global vocab instance
from src.psgm.dataset import PeptideHLADataset, condition_collate
from src.psgm.model import HLAGenerator, Discriminator
from src.psgm.mapper import HLAMapper
from src.psgm.utils import get_position_masks, generate, evaluate_results

def set_seed(seed):
    """Sets random seeds for reproducibility."""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)
    # os.environ['PYTHONHASHSEED'] = str(seed) # Optional: set PYTHONHASHSEED

def train():
    """
    Main training function for the PSGM (HLA Generator and Discriminator).
    """
    device = torch.device(Config.DEVICE)
    set_seed(Config.SEED) # Set seed at the start of training

    print(f"Using device: {device}")
    print(f"Loading data from: {Config.TRAIN_DATA_PATH}")

    # Load and prepare dataset
    try:
        df = pd.read_csv(Config.TRAIN_DATA_PATH)
    except FileNotFoundError:
        print(f"Error: Training data not found at {Config.TRAIN_DATA_PATH}. Please check the path in config/psgm_config.py.")
        return

    dataset = PeptideHLADataset(df)
    
    # Randomly split dataset into training, validation, and test sets
    train_size = int(0.8 * len(dataset))
    val_size = int(0.1 * len(dataset))
    test_size = len(dataset) - train_size - val_size
    
    # Ensure reproducibility of random_split
    generator_for_split = torch.Generator().manual_seed(Config.SEED)
    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size], generator=generator_for_split)

    print(f"Dataset split: Train {len(train_set)}, Val {len(val_set)}, Test {len(test_set)}")

    # Save split datasets to CSV files for later use (e.g., evaluation)
    dataset.df.iloc[train_set.indices].to_csv(Config.TRAIN_SPLIT_SAVE_PATH, index=False)
    dataset.df.iloc[val_set.indices].to_csv(Config.VAL_SPLIT_SAVE_PATH, index=False)
    dataset.df.iloc[test_set.indices].to_csv(Config.TEST_SPLIT_SAVE_PATH, index=False)
    print(f"Split datasets saved: {Config.TRAIN_SPLIT_SAVE_PATH}, {Config.VAL_SPLIT_SAVE_PATH}, {Config.TEST_SPLIT_SAVE_PATH}")
    
    # Initialize models
    generator = HLAGenerator(len(vocab)).to(device)
    discriminator = Discriminator(len(vocab)).to(device)
    
    # Define optimizers
    optimizer_g = torch.optim.AdamW(generator.parameters(), lr=Config.LEARNING_RATE)
    optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=Config.LEARNING_RATE)
    
    # Define loss functions
    criterion_ce = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx) # For Generator's reconstruction
    criterion_bce = nn.BCEWithLogitsLoss() # For Discriminator, use BCEWithLogitsLoss for numerical stability
    
    # Create DataLoaders
    train_loader = DataLoader(train_set, batch_size=Config.BATCH_SIZE, 
                              shuffle=True, collate_fn=condition_collate)
    val_loader = DataLoader(val_set, batch_size=Config.BATCH_SIZE * 2, # Larger batch size for validation
                            collate_fn=condition_collate)

    # Early stopping parameters
    patience_counter = 0
    best_val_loss = float('inf')

    print("\n--- Starting Training ---")
    for epoch in range(Config.NUM_EPOCHS):
        generator.train()
        discriminator.train()
        train_loss_g_epoch = 0
        train_loss_d_epoch = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.NUM_EPOCHS} Training")
        for batch in pbar:
            peptide = batch['peptide'].to(device)
            hla = batch['hla'].to(device)
            
            # ---------------------
            #  Train Discriminator
            # ---------------------
            optimizer_d.zero_grad()
            
            # Real samples
            real_pred = discriminator(hla)
            loss_d_real = criterion_bce(real_pred, torch.ones_like(real_pred)) # Label 1 for real

            # Fake samples (generated by Generator, detached from its graph)
            # The input to generator's decoder is hla[:, :-1], and it predicts hla[:, 1:]
            fake_hla_logits_gen = generator(peptide, hla[:, :-1])
            
            # For discriminator input, we need concrete tokens. Sample from logits.
            # Using argmax for simplicity here, but sampling could be more complex.
            # We also need to handle padding during sequence generation and in fake_seq.
            # If `Config.HLA_SEQ_LEN` is the actual target length, then `fake_seq` needs to be this length.
            # The `generate` function handles step-by-step generation with masks.
            # For direct `argmax` from `fake_hla_logits_gen`, it's `hla_len-1`.
            
            # The original code's `fake_seq = torch.argmax(fake_hla_logits, dim=-1)` uses `fake_hla_logits`
            # which is the output of `generator(peptide, hla[:, :-1])`.
            # This means `fake_seq` will have shape `[batch_size, Config.HLA_SEQ_LEN - 1]`.
            # However, `discriminator(hla)` expects `hla` of shape `[batch_size, Config.HLA_SEQ_LEN]`.
            # To match shapes, either:
            # 1. Generate full sequence (e.g., using a start token or a simple generation loop)
            # 2. Pad/truncate `hla` for `real_pred` or pad `fake_seq` for `fake_pred`
            # The simplest for training GAN where shapes must match:
            # Let's assume Discriminator always receives `Config.HLA_SEQ_LEN` length sequence.
            # For fake, we need to generate one of `Config.HLA_SEQ_LEN` length.
            # The `generate` function already does this for a single peptide.
            # For batch training, we might need a batch generation step.

            # Re-evaluating based on original code `fake_hla_logits = generator(peptide, hla[:, :-1])`
            # then `fake_seq = torch.argmax(fake_hla_logits, dim=-1)`.
            # This means `fake_seq` is `hla_len - 1` length.
            # And `real_pred = discriminator(hla)` uses `hla` of `hla_len`.
            # This is an inconsistency that needs addressing.
            # A common approach for GANs with sequence generation:
            # - Generator generates `hla_len` tokens (e.g., auto-regressively from a start token).
            # - Discriminator then evaluates these generated full sequences.

            # To align with the original provided code's apparent logic for D input:
            # `hla` is typically `[batch_size, max_len]`.
            # `generator(peptide, hla[:, :-1])` implies it's predicting the next token in `hla`.
            # `fake_seq = torch.argmax(fake_hla_logits_gen, dim=-1)` means fake_seq is `[batch_size, max_len-1]`.
            # This still implies a mismatch.
            # For consistency, let's make `fake_seq` match the length of `hla` for the Discriminator.
            # A simple workaround: if `hla` has start tokens and `hla[:,:-1]` is input, then `fake_hla_logits_gen`
            # corresponds to `hla[:,1:]`. We can append a dummy start token or directly use `hla[:,1:]` as real.
            # Or, generate full sequences:
            
            # Let's adapt the training loop: Generate *full* HLA sequences for discriminator.
            # This will require slight modification of generator's forward or using generate utility.
            # For simplicity, if the original GAN setup expects `HLA_SEQ_LEN - 1` for fake_seq
            # and `HLA_SEQ_LEN` for real_seq for Discriminator:
            # This implies Discriminator might be processing sequences of different lengths.
            # This is problematic for TransformerEncoder if no padding mask is handled carefully.
            
            # Given `Discriminator` is `nn.TransformerEncoder` and it calls `seq.mean(dim=1)`
            # meaning it expects fixed length or a mask that correctly pools.
            # Let's assume `discriminator` should operate on `Config.HLA_SEQ_LEN` length.
            
            # Revised generation for Discriminator:
            # Generate a "start token" + generated sequence to match `hla` length.
            # Or, directly use `hla[:, 1:]` for Discriminator input if the first token is always 'Y'
            # and not generated by the generator's main loop.
            
            # The `generate` utility function in `utils.py` actually generates `Config.HLA_SEQ_LEN` tokens.
            # For batch training, we need a batch version of this.
            # For simplicity for this refactoring, let's assume `generator` returns `Config.HLA_SEQ_LEN` length sequences for discriminator.
            # Let's fix this here by ensuring `fake_seq` is `Config.HLA_SEQ_LEN`.

            # Option 1: Regenerate fake sequences of full length from generator
            # This would involve a new function for batch generation for GANs
            # For now, let's stick to the original logic as closely as possible,
            # but acknowledging the shape mismatch as a potential issue unless the Discriminator handles it.
            # The problem stated in the code: `fake_seq` is `HLA_SEQ_LEN - 1`. `hla` is `HLA_SEQ_LEN`.
            # This implies the Discriminator must handle variable length inputs, or there's an implicit slicing.

            # Based on the original code's Discriminator and its `forward(self, seq)`:
            # `embed(seq)` and `encoder(embedded)`.
            # If `seq` is `[batch_size, seq_len]`, the embedding and encoder work for variable `seq_len`.
            # `pooled = encoded.mean(dim=1)` for variable `seq_len` is also valid (mean over varying length).
            # So, the Discriminator can handle `seq_len` of `Config.HLA_SEQ_LEN` AND `Config.HLA_SEQ_LEN - 1`.
            # The key is `hla[:, :-1]` for generator input, and `hla[:, 1:]` for its loss target.
            # And for fake_seq, `torch.argmax` from `fake_hla_logits_gen` is used.
            # This `fake_seq` should then be prepended with the 'Y' token to match `hla` length for Discriminator.

            generated_hla_logits = generator(peptide, hla[:, :-1]) # [batch_size, HLA_SEQ_LEN-1, vocab_size]
            
            # Take argmax to get discrete tokens for discriminator
            # This `fake_seq_body` is `[batch_size, HLA_SEQ_LEN - 1]`
            fake_seq_body = torch.argmax(generated_hla_logits.detach(), dim=-1)
            
            # Prepend 'Y' token to match the length of real HLA sequences for Discriminator
            # Assuming 'Y' is the fixed start token as per `get_position_masks` and `generate`
            y_token = vocab.token_to_idx['Y']
            fake_start_token = torch.full((fake_seq_body.size(0), 1), y_token, dtype=torch.long, device=device)
            fake_hla_for_d = torch.cat([fake_start_token, fake_seq_body], dim=1) # [batch_size, HLA_SEQ_LEN]

            fake_pred = discriminator(fake_hla_for_d.detach()) # Detach for D training
            loss_d_fake = criterion_bce(fake_pred, torch.zeros_like(fake_pred)) # Label 0 for fake
            
            loss_d = (loss_d_real + loss_d_fake) / 2
            loss_d.backward()
            optimizer_d.step()
            train_loss_d_epoch += loss_d.item()

            # ---------------------
            #  Train Generator
            # ---------------------
            optimizer_g.zero_grad()
            
            # Reconstruction loss (Cross-Entropy Loss)
            # Generator predicts hla[:, 1:] from hla[:, :-1]
            loss_g_ce = criterion_ce(generated_hla_logits.reshape(-1, len(vocab)), hla[:, 1:].reshape(-1))

            # Adversarial loss (Generator tries to fool Discriminator)
            fake_pred_g = discriminator(fake_hla_for_d) # Now fake_hla_for_d is NOT detached
            loss_g_adv = criterion_bce(fake_pred_g, torch.ones_like(fake_pred_g)) # Generator wants D to classify as real

            loss_g = loss_g_ce + Config.LAMBDA_ADV * loss_g_adv
            loss_g.backward()
            torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0) # Clip gradients to prevent exploding gradients
            optimizer_g.step()
            train_loss_g_epoch += loss_g.item()
            
            pbar.set_postfix({'G_loss': loss_g.item(), 'D_loss': loss_d.item()})
        
        avg_train_loss_g = train_loss_g_epoch / len(train_loader)
        avg_train_loss_d = train_loss_d_epoch / len(train_loader)

        # Validation phase
        generator.eval()
        val_loss_ce = 0
        with torch.no_grad():
            for batch_val in tqdm(val_loader, desc=f"Epoch {epoch+1}/{Config.NUM_EPOCHS} Validation"):
                peptide_val = batch_val['peptide'].to(device)
                hla_val = batch_val['hla'].to(device)
                
                outputs_val = generator(peptide_val, hla_val[:, :-1])
                loss_ce_val = criterion_ce(outputs_val.reshape(-1, len(vocab)), hla_val[:, 1:].reshape(-1))
                val_loss_ce += loss_ce_val.item()
        
        avg_val_loss_ce = val_loss_ce / len(val_loader)
        
        print(f"\nEpoch {epoch+1} Summary:")
        print(f"  Train G Loss: {avg_train_loss_g:.4f}, Train D Loss: {avg_train_loss_d:.4f}")
        print(f"  Validation CE Loss: {avg_val_loss_ce:.4f}")

        # Early stopping logic (based on Generator's CE loss on validation set)
        if avg_val_loss_ce < best_val_loss - Config.MIN_DELTA:
            best_val_loss = avg_val_loss_ce
            patience_counter = 0
            torch.save(generator.state_dict(), Config.MODEL_SAVE_PATH)
            print(f"New best model saved with validation CE loss: {avg_val_loss_ce:.4f}")
        else:
            patience_counter += 1
            print(f"Validation CE loss did not improve. Counter: {patience_counter}/{Config.PATIENCE}")
            if patience_counter >= Config.PATIENCE:
                print(f"Early stopping triggered at epoch {epoch+1}")
                break

    print("\n--- Training Complete ---")

# ----------------------
# Main Execution Block
# ----------------------
if __name__ == "__main__":
    train() # Run the training process

    # After training, load the best model and perform evaluation
    device = torch.device(Config.DEVICE)
    
    # Initialize Generator and load the best saved weights
    model = HLAGenerator(len(vocab)).to(device)
    try:
        model.load_state_dict(torch.load(Config.MODEL_SAVE_PATH, map_location=device))
        print(f"\nLoaded best generator model from: {Config.MODEL_SAVE_PATH}")
    except FileNotFoundError:
        print(f"\nError: Best model not found at {Config.MODEL_SAVE_PATH}. Cannot perform evaluation.")
        print("Please ensure training was successful and saved the model.")
        exit() # Exit if model not found

    # Initialize HLA Mapper
    mapper = HLAMapper(Config.HLA_DB_PATH)

    # Get position masks for generation
    position_masks = get_position_masks()

    # Load the test peptides from the split test set CSV
    try:
        test_df = pd.read_csv(Config.TEST_SPLIT_SAVE_PATH)
        test_peptides = test_df['peptide'].unique().tolist() # Get unique peptides from test set
        print(f"Loaded {len(test_peptides)} unique peptides from test set for evaluation.")
    except FileNotFoundError:
        print(f"Error: Test split data not found at {Config.TEST_SPLIT_SAVE_PATH}. Cannot perform evaluation.")
        exit()

    # Evaluate results and save to CSV
    results_df = evaluate_results(model, mapper, device, test_peptides, position_masks)
    print("Evaluation complete. Results saved.")
